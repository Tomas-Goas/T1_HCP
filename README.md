# üßÆ T1_HCP

## üìä Resultados

### üîπ Resultados de referencia (versi√≥n secuencial)

| Par√°metro | Valor |
|------------|--------|
| **Rango evaluado** | 2 ‚Üí 400,000,000 |
| **Primos encontrados** | 21,336,326 |
| **Tiempo de ejecuci√≥n** | ‚è±Ô∏è 260.809 segundos |

### üîπ Resultados Version Paralela

#### Schedule(static)

| Chunks | Par√°metro | 8 Hilos | 16 Hilos |
|:-------|:-----------|:--------|:----------|
| **10** | **Tiempo de ejecuci√≥n** | ‚è±Ô∏è 47.5133 s | ‚è±Ô∏è 37.9286 s |
|  | **Speedup** | 4.46√ó | 6.87√ó |
| **1000** | **Tiempo de ejecuci√≥n** | ‚è±Ô∏è 48.044 s | ‚è±Ô∏è 39.520 s |
|  | **Speedup** | 4.41√ó | 6.60√ó |
| **100000** | **Tiempo de ejecuci√≥n** | ‚è±Ô∏è 48.5659 s | ‚è±Ô∏è 40.9047 s |
|  | **Speedup** | 4.36√ó | 6.37√ó |

#### Schedule(dynamic)

| Chunks | Par√°metro | 8 Hilos | 16 Hilos |
|:-------|:-----------|:--------|:----------|
| **10** | **Tiempo de ejecuci√≥n** | ‚è±Ô∏è 45.2478 s | ‚è±Ô∏è 39.2581 s |
|  | **Speedup** | 4.70√ó | 6.64√ó |
| **1000** | **Tiempo de ejecuci√≥n** | ‚è±Ô∏è 48.0436 s | ‚è±Ô∏è 39.752 s |
|  | **Speedup** | 4.41√ó | 6.56√ó |
| **100000** | **Tiempo de ejecuci√≥n** | ‚è±Ô∏è 47.4016 s | ‚è±Ô∏è 36.3698 s |
|  | **Speedup** | 4.47√ó | 7.17√ó |

---

#### Schedule(guided)

| Chunks | Par√°metro | 8 Hilos | 16 Hilos |
|:-------|:-----------|:--------|:----------|
| **10** | **Tiempo de ejecuci√≥n** | ‚è±Ô∏è 44.956 s | ‚è±Ô∏è 38.5945 s |
|  | **Speedup** | 4.73√ó | 6.76√ó |
| **1000** | **Tiempo de ejecuci√≥n** | ‚è±Ô∏è 46.535 s | ‚è±Ô∏è 35.820 s |
|  | **Speedup** | 5.60√ó | 7.28√ó |
| **100000** | **Tiempo de ejecuci√≥n** | ‚è±Ô∏è 45.042 s | ‚è±Ô∏è 36.4544 s |
|  | **Speedup** | 4.72√ó | 7.15√ó |


### üìà Gr√°fico Comparativo de Tiempos

Los siguientes resultados ilustran el comportamiento de los tres *schedules* (`static`, `dynamic`, `guided`) con diferentes cantidades de *chunks* y hilos.

![Gr√°fico de Schedules](assets/grafico_schedules.png)

## ‚öôÔ∏è Gr√°ficos de Speedup y Eficiencia vs N¬∫ de Procesadores

A continuaci√≥n se presentan los gr√°ficos generados a partir de los resultados experimentales para cada tipo de *schedule*. Estos gr√°ficos permiten analizar c√≥mo var√≠a la aceleraci√≥n (Speedup) y la eficiencia del paralelismo al aumentar el n√∫mero de procesadores.

---

### üî∏ Schedule Static
![Speedup y Eficiencia - Static](graficos/Speedup_Eficiencia_Static.png)

---

### üî∏ Schedule Dynamic
![Speedup y Eficiencia - Dynamic](graficos/Speedup_Eficiencia_Dynamic.png)

---

### üî∏ Schedule Guided
![Speedup y Eficiencia - Guided](graficos/Speedup_Eficiencia_Guided.png)

---

### üß† An√°lisis de Speedup y Eficiencia vs N¬∞ Procesadores

En todos los schedules, se observa que el speedup crece al aumentar el n√∫mero de procesadores, lo cual confirma que el algoritmo se beneficia del paralelismo. Sin embargo, la eficiencia disminuye conforme aumentan los hilos, lo que refleja la p√©rdida de rendimiento relativo debido a la sobrecarga de sincronizaci√≥n, el costo de dividir las tareas y la parte secuencial del c√≥digo, lo que es esperado en sistemas paralelos reales debido a la ley de Amdahl como vimos en clases.

El schedule static asigna bloques de iteraciones de tama√±o fijo a cada hilo, obteniendo speedups desde 4.3x a 6.8x y una eficiencia que cae de 0,55 a 0,43 al duplicar el n√∫mero de hilos. Esto puede deberse a que los n√∫meros primos no se distribuyen uniformemente entre los 400 millones a indagar pues los primeros rangos tienen m√°s trabajo al frecuentar m√°s primos que al final, causando que algunos hilos puedan terminar antes esperando el procesamiento de otros. Se puede apreciar que este efecto empeora con un mayor tama√±o de chunks pues cada hilo recibe menos bloques con peor granularidad, siendo chunk=10 el caso con un mayor rendimiento para este schedule.

El schedule dynamic por su parte comienza como el static con una asignaci√≥n fija pero luego reasigna bloques de trabajo a medida que los hilos terminan mejorando el balance de carga. Se puede ver que el speedup mejora ligeramente frente al static al distribuir el trabajo de cada hilo seg√∫n su disponibilidad pero con el sacrificio de overhead al tener que reasignar estos bloques din√°micamente y asegurar sincronizaci√≥n. Por esta raz√≥n, se puede observar que con chunk=10 la ganancia de balance no compensa el costo de reasignaci√≥n, en cambio con mayor tama√±o de chunks se logra un equilibrio m√°s eficiente con un speedup de 7.17x ya que el overhead se equilibra con la flexibilidad de reasignaci√≥n ante irregularidades en la b√∫squeda.

Por √∫ltimo, el schedule guided asigna inicialmente bloques grandes y luego los va reduciendo din√°micamente, combinando ambas estrategias antes mencionadas. Se pueden apreciar speedups m√°s altos como 7.28x con 16 hilos y chunk=1000, con este tama√±o inicial de chunk se obtienen los mejores resultados tanto en speedup como en eficiencia lo que sugiere ser un tama√±o ideal para la b√∫squeda de primos que son m√°s frecuentes en un comienzo que al final de la b√∫squeda. Un menor tama√±o de chunks aumenta mucho la fragmentaci√≥n causando mayor sobrecarga, mientras que uno grande no aprovecha efectivamente la estrategia de distribuci√≥n de bloques.
